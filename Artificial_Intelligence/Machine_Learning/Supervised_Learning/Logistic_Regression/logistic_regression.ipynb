{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - H·ªìi quy Logistic\n",
    "\n",
    "## 1. Gi·ªõi thi·ªáu\n",
    "Logistic Regression (H·ªìi quy Logistic) l√† m·ªôt m√¥ h√¨nh h·ªçc c√≥ gi√°m s√°t, ƒë∆∞·ª£c s·ª≠ d·ª•ng ch·ªß y·∫øu cho b√†i to√°n **ph√¢n lo·∫°i nh·ªã ph√¢n (Binary Classification)**. \n",
    "M·∫∑c d√π c√≥ t√™n \"h·ªìi quy\", nh∆∞ng Logistic Regression th·ª±c ch·∫•t l√† m·ªôt thu·∫≠t to√°n **ph√¢n lo·∫°i**, kh√¥ng ph·∫£i h·ªìi quy.\n",
    "\n",
    "## 2. C√°ch ho·∫°t ƒë·ªông c·ªßa Logistic Regression\n",
    "Qu√° tr√¨nh ho·∫°t ƒë·ªông c·ªßa Logistic Regression g·ªìm c√°c b∆∞·ªõc ch√≠nh:\n",
    "\n",
    "1. **T√≠nh t·ªïng tr·ªçng s·ªë tuy·∫øn t√≠nh c·ªßa ƒë·∫ßu v√†o**\n",
    "   - S·ª≠ d·ª•ng c√¥ng th·ª©c:  \n",
    "     $$ z = w_1x_1 + w_2x_2 + ... + w_nx_n + b $$\n",
    "     - Trong ƒë√≥:\n",
    "       - \\( x_i \\) l√† c√°c ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o.\n",
    "       - \\( w_i \\) l√† c√°c tr·ªçng s·ªë (weights).\n",
    "       - \\( b \\) l√† bias (h·ªá s·ªë t·ª± do).\n",
    "\n",
    "2. **√Åp d·ª•ng h√†m sigmoid ƒë·ªÉ chu·∫©n h√≥a ƒë·∫ßu ra v·ªÅ kho·∫£ng [0,1]**\n",
    "   - C√¥ng th·ª©c c·ªßa h√†m **Sigmoid**:\n",
    "     $$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "   - K·∫øt qu·∫£ ƒë·∫ßu ra l√† x√°c su·∫•t thu·ªôc v√†o l·ªõp 1.\n",
    "   - N·∫øu \\( \\sigma(z) \\geq 0.5 \\), m·∫´u d·ªØ li·ªáu ƒë∆∞·ª£c g√°n nh√£n l√† **1**, ng∆∞·ª£c l·∫°i g√°n nh√£n **0**.\n",
    "\n",
    "3. **H√†m m·∫•t m√°t (Loss Function)**\n",
    "   - H√†m m·∫•t m√°t c·ªßa Logistic Regression l√† **Binary Cross-Entropy (Log Loss)**:\n",
    "     $$ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] $$\n",
    "     - Trong ƒë√≥:\n",
    "       - \\( y_i \\) l√† nh√£n th·ª±c t·∫ø.\n",
    "       - \\( \\hat{y}_i \\) l√† x√°c su·∫•t d·ª± ƒëo√°n.\n",
    "       - \\( m \\) l√† s·ªë l∆∞·ª£ng m·∫´u d·ªØ li·ªáu.\n",
    "\n",
    "4. **T·ªëi ∆∞u h√≥a tham s·ªë b·∫±ng Gradient Descent**\n",
    "   - C·∫≠p nh·∫≠t tr·ªçng s·ªë theo c√¥ng th·ª©c:\n",
    "     $$ w_j = w_j - \\alpha \\frac{\\partial J}{\\partial w_j} $$\n",
    "   - Trong ƒë√≥:\n",
    "     - \\( \\alpha \\) l√† t·ªëc ƒë·ªô h·ªçc (learning rate).\n",
    "     - \\( \\frac{\\partial J}{\\partial w_j} \\) l√† ƒë·∫°o h√†m c·ªßa h√†m m·∫•t m√°t theo tr·ªçng s·ªë \\( w_j \\).\n",
    "\n",
    "## 3. C√°c c√¥ng th·ª©c to√°n h·ªçc quan tr·ªçng\n",
    "\n",
    "### 3.1 H√†m Sigmoid\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "üëâ Gi√∫p chuy·ªÉn ƒë·ªïi ƒë·∫ßu ra v·ªÅ kho·∫£ng **[0,1]** ƒë·ªÉ bi·ªÉu di·ªÖn x√°c su·∫•t.\n",
    "\n",
    "### 3.2 H√†m m·∫•t m√°t Binary Cross-Entropy\n",
    "$$ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] $$\n",
    "üëâ ƒê∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒëo l∆∞·ªùng sai s·ªë gi·ªØa d·ª± ƒëo√°n v√† th·ª±c t·∫ø.\n",
    "\n",
    "### 3.3 C·∫≠p nh·∫≠t tr·ªçng s·ªë b·∫±ng Gradient Descent\n",
    "$$ w_j = w_j - \\alpha \\frac{\\partial J}{\\partial w_j} $$\n",
    "üëâ Gi√∫p t·ªëi ∆∞u m√¥ h√¨nh b·∫±ng c√°ch gi·∫£m d·∫ßn sai s·ªë.\n",
    "\n",
    "## 4. ·ª®ng d·ª•ng th·ª±c t·∫ø c·ªßa Logistic Regression\n",
    "- **Ph√¢n lo·∫°i email spam**\n",
    "- **Ch·∫©n ƒëo√°n b·ªánh t·∫≠t (V√≠ d·ª•: d·ª± ƒëo√°n ung th∆∞ d·ª±a tr√™n d·ªØ li·ªáu y t·∫ø)**\n",
    "- **Ph√°t hi·ªán gian l·∫≠n t√†i ch√≠nh**\n",
    "- **D·ª± ƒëo√°n kh√°ch h√†ng c√≥ r·ªùi b·ªè d·ªãch v·ª• hay kh√¥ng (Customer Churn Prediction)**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
